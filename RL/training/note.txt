REINFORCE double integrator1d

Decent training parameters:
env = DoubleIntegrator1D(
        delta_t=0.05, target_x=0, x_bound=[-10, 10], v_bound=[-5, 5], x_epsilon=0.5, vx_epsilon=0.2, debug=True)
policy = DoubleIntegratorPolicy(state_dim=2, action_dim=40, hidden_dims=[16, 64], action_range=[-1, 1])
reinforce = REINFORCE(env, policy, optimizer, num_episodes=1000, max_steps=100, gamma=0.99)

double_integrator_REINFORCE_Oscillate training parameters:
env = DoubleIntegrator1D(
        delta_t=0.05, target_x=0, x_bound=[-10, 10], v_bound=[-5, 5], x_epsilon=0.5, vx_epsilon=0.2, debug=True)
policy = DoubleIntegratorPolicy(state_dim=2, action_dim=40, hidden_dims=[16, 64], action_range=[-1, 1])
reinforce = REINFORCE(env, policy, optimizer, num_episodes=1000, max_steps=100, gamma=0.99)


Findings:

REINFORCE with double integrator1d

1. Oscilliatory behavior: We can observe there is Oscilliatory behavior given an initial condition.
This means the policy is not optimal, and probably the policy does not care as long as it is staying
close to the goal.

2. Maximum episode steps: With episode step too short, the learning algorithm may not reach the boundary
of the environment, which will penalize the boundary violation. So need to make sure that episode length
is long enough so the algorithm can explore these regions.

3. Time penalty: We may need to penalize the time the point mass spends not reaching the region, so there
is an incentive to reach it earlier.

4. Behavior that leads to boundary: During inference, so trajectory leads to boundary violation. Maybe the
boundary violation is not penalized enough so the algorithm does not know the seriousness of such behavior.

Potential mitigation techniques:
Oscilliatory behavior:
1. Need to penalize overshooting behavior, such as rapid change in control.
2. Dampen the control change, through a filter or something.

Boundary violation:
1. Lengthen the episode.
2. Stronger penalty for violation.
3. Curriculum Learning: Progressively increase the episode length.
4. Soft boundary penalty: Not stopping the episode during boundary violation, instead use non-linear penalty function.

Time penalty:
1. Constant cumulative time penalty.

Algorithmic consideration:
1. Policy network layer depth and width change.
2. Policy network random initialization.
